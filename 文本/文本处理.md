###1 文本分类

####1.1 语料获取

* 公开数据集
* 爬虫爬取（需要过滤标签、链接等非文本内容）

####1.2 文本预处理

#####1.2.1 分词

* 英文：利用空格与标点进行划分
* 中文：利用 jieba 分词进行划分，尝试 N-gram 的方法，其中 N 越大，特征数量就越多，分类速度就越低。通常工程中采用 N 为 1-3 的 N-gram 即可。

#####1.2.2 词性标注

对划分出的词，标注其在句子中的含义

#####1.2.3 文本过滤

* 停用词（大量存在，不能反映文本的含义）
* 标点符号、特殊符号、虚词

#####1.2.4 长串的数字或字母

通常代表手机号、车牌号、用户名等内容，非特定情形下可以去除或转换为其他特征。

#####1.2.5 无意义文本

广告内容、版权信息、个性签名等

#####1.2.6 变形词识别与替换

对于不良、敏感的词汇用不敏感的词汇进行代替，常用特殊符号、同音近型等等进行替换。

####1.3 文本特征构建

将文本符号转变为数字

#####1.3.1 词袋表示（bag of word, BOW）

不考虑词语在句中的位置，直接统计词语与符号的出现次数。

* count：先将关键词作为文本特征，然后再统计词频。
* tf-idf：某个词语在一篇文章中出现的次数高，在其他文章中出现的次数很少，就越能代表这篇文章。先计算逆文档频率（IDF），再统计词频（TF），最后两者相乘即可。

#####1.3.2 词向量表示（word embedding）

维度低、节省计算资源、捕获了相对位置的语义信息

* word2vec
* glove

####1.4 特征选择

#####1.4.1 特征选择

* 文档频率（Document Frequency, DF）
* 互信息（Mutual Information, MI）
* 信息增益（Information Gain, IG）
* 卡方检验（Chi-square）：优先选择方差大的特征
* 加权对数似然比（Weighted Log Likelihood Ration, WLLR）

#####1.4.2 特征降维

LDA（线性特征抽取），PCA（主成分分析），FA（因子分析），SVD（奇异值分解），NMF（非负矩阵分解），LSI或者LSA（潜在语义分析）

####1.5 评价指标

准确率，错误率，精确度，召回率

### 参考文档：

[1] http://www.ruanyifeng.com/blog/2013/03/tf-idf.html

[2] https://www.jianshu.com/p/a72b6a0cf895
